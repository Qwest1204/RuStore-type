{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T03:55:04.585846Z",
     "start_time": "2025-10-01T03:54:50.893838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from model.main_model import Classifier\n",
    "from data.dataset import RuStoreDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd"
   ],
   "id": "b995cb86c139d8e5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniilogorodnikov/PycharmProjects/RuStore-type/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/daniilogorodnikov/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/7710840340a098cfb869c4f65e87cf2b1b70caca/modeling_hf_nomic_bert.py:1634: UserWarning: Install Nomic's megablocks fork for better speed: `pip install git+https://github.com/nomic-ai/megablocks.git`\n",
      "  warnings.warn(\"Install Nomic's megablocks fork for better speed: \" +\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T03:55:05.657242Z",
     "start_time": "2025-10-01T03:55:04.678689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train = pd.read_csv(\"data/train.tsv\", sep='\\t').dropna()\n",
    "\n",
    "dataset = RuStoreDataset(train)\n",
    "batch_size = 2 # Adjust as needed\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ],
   "id": "5dfa83e79681f0d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T03:55:05.695088Z",
     "start_time": "2025-10-01T03:55:05.662902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_dim = 2304  # As per the model\n",
    "num_classes = 394  # As per the model\n",
    "\n",
    "model = Classifier(input_dim=input_dim, num_classes=num_classes)\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # Adjust learning rate as needed\n",
    "# Optional: Scheduler\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 10  # Adjust as needed"
   ],
   "id": "8522cfa1b797df8a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T03:55:05.714060Z",
     "start_time": "2025-10-01T03:55:05.711337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ],
   "id": "d7b0e39a86f5bc8c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T03:55:05.730867Z",
     "start_time": "2025-10-01T03:55:05.727814Z"
    }
   },
   "cell_type": "code",
   "source": "from tqdm import tqdm",
   "id": "230b78d27acd2c9c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T03:56:35.286843Z",
     "start_time": "2025-10-01T03:55:05.742296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        inputs = inputs.to(device)  # Inputs shape: (bs, 3, 768) or similar, but model reshapes\n",
    "        labels = labels.to(device)  # Labels: (bs,)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get logits (without softmax, for loss)\n",
    "        logits = model.get_logits(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track loss and accuracy\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        print(correct)\n",
    "        print(running_loss)\n",
    "\n",
    "    # Optional: Step the scheduler\n",
    "    # scheduler.step()\n",
    "\n",
    "    # Epoch statistics\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100 * correct / len(train_loader)\n",
    "\n",
    "    print(f\"epoch_loss: {epoch_loss}, epoch_acc: {epoch_acc}\")\n",
    "    torch.save(model.state_dict(), f\"models/{epoch}.pth\")"
   ],
   "id": "4178ffe97f46b0b8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26747 [00:00<?, ?it/s]/Users/daniilogorodnikov/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/7710840340a098cfb869c4f65e87cf2b1b70caca/modeling_hf_nomic_bert.py:1634: UserWarning: Install Nomic's megablocks fork for better speed: `pip install git+https://github.com/nomic-ai/megablocks.git`\n",
      "  warnings.warn(\"Install Nomic's megablocks fork for better speed: \" +\n",
      "/Users/daniilogorodnikov/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/7710840340a098cfb869c4f65e87cf2b1b70caca/modeling_hf_nomic_bert.py:1634: UserWarning: Install Nomic's megablocks fork for better speed: `pip install git+https://github.com/nomic-ai/megablocks.git`\n",
      "  warnings.warn(\"Install Nomic's megablocks fork for better speed: \" +\n",
      "/Users/daniilogorodnikov/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/7710840340a098cfb869c4f65e87cf2b1b70caca/modeling_hf_nomic_bert.py:1634: UserWarning: Install Nomic's megablocks fork for better speed: `pip install git+https://github.com/nomic-ai/megablocks.git`\n",
      "  warnings.warn(\"Install Nomic's megablocks fork for better speed: \" +\n",
      "/Users/daniilogorodnikov/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/7710840340a098cfb869c4f65e87cf2b1b70caca/modeling_hf_nomic_bert.py:1634: UserWarning: Install Nomic's megablocks fork for better speed: `pip install git+https://github.com/nomic-ai/megablocks.git`\n",
      "  warnings.warn(\"Install Nomic's megablocks fork for better speed: \" +\n",
      "  0%|          | 0/26747 [01:28<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/daniilogorodnikov/PycharmProjects/RuStore-type/venv/lib/python3.13/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/Users/daniilogorodnikov/PycharmProjects/RuStore-type/venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/Users/daniilogorodnikov/PycharmProjects/RuStore-type/data/dataset.py\", line 24, in __getitem__\n    app_name_emb = embedded([X['app_name']])\n  File \"/Users/daniilogorodnikov/PycharmProjects/RuStore-type/model/embedding.py\", line 18, in embedded\n    model_output = model(**encoded_input)\n  File \"/Users/daniilogorodnikov/PycharmProjects/RuStore-type/venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/Users/daniilogorodnikov/PycharmProjects/RuStore-type/venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/Users/daniilogorodnikov/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/7710840340a098cfb869c4f65e87cf2b1b70caca/modeling_hf_nomic_bert.py\", line 1905, in forward\n    hidden_states = self.embeddings(\n        input_ids=input_ids,\n    ...<2 lines>...\n        inputs_embeds=inputs_embeds,\n    )\n  File \"/Users/daniilogorodnikov/PycharmProjects/RuStore-type/venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/Users/daniilogorodnikov/PycharmProjects/RuStore-type/venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/Users/daniilogorodnikov/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/7710840340a098cfb869c4f65e87cf2b1b70caca/modeling_hf_nomic_bert.py\", line 1010, in forward\n    embeddings = self.word_embeddings(input_ids)\n  File \"/Users/daniilogorodnikov/PycharmProjects/RuStore-type/venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/Users/daniilogorodnikov/PycharmProjects/RuStore-type/venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/Users/daniilogorodnikov/PycharmProjects/RuStore-type/venv/lib/python3.13/site-packages/torch/nn/modules/sparse.py\", line 192, in forward\n    return F.embedding(\n           ~~~~~~~~~~~^\n        input,\n        ^^^^^^\n    ...<5 lines>...\n        self.sparse,\n        ^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/daniilogorodnikov/PycharmProjects/RuStore-type/venv/lib/python3.13/site-packages/torch/nn/functional.py\", line 2546, in embedding\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Placeholder storage has not been allocated on MPS device!\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      5\u001B[39m correct = \u001B[32m0\u001B[39m\n\u001B[32m      6\u001B[39m total = \u001B[32m0\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtqdm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Inputs shape: (bs, 3, 768) or similar, but model reshapes\u001B[39;49;00m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Labels: (bs,)\u001B[39;49;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RuStore-type/venv/lib/python3.13/site-packages/tqdm/std.py:1181\u001B[39m, in \u001B[36mtqdm.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1178\u001B[39m time = \u001B[38;5;28mself\u001B[39m._time\n\u001B[32m   1180\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1181\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   1182\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\n\u001B[32m   1183\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Update and possibly print the progressbar.\u001B[39;49;00m\n\u001B[32m   1184\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;49;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RuStore-type/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:734\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    731\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    732\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    733\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m734\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    735\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    736\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    737\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    738\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    739\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    740\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RuStore-type/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1516\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1514\u001B[39m worker_id = \u001B[38;5;28mself\u001B[39m._task_info.pop(idx)[\u001B[32m0\u001B[39m]\n\u001B[32m   1515\u001B[39m \u001B[38;5;28mself\u001B[39m._rcvd_idx += \u001B[32m1\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1516\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_process_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mworker_id\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RuStore-type/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1551\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._process_data\u001B[39m\u001B[34m(self, data, worker_idx)\u001B[39m\n\u001B[32m   1549\u001B[39m \u001B[38;5;28mself\u001B[39m._try_put_index()\n\u001B[32m   1550\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ExceptionWrapper):\n\u001B[32m-> \u001B[39m\u001B[32m1551\u001B[39m     \u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1552\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RuStore-type/venv/lib/python3.13/site-packages/torch/_utils.py:769\u001B[39m, in \u001B[36mExceptionWrapper.reraise\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    765\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m    766\u001B[39m     \u001B[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001B[39;00m\n\u001B[32m    767\u001B[39m     \u001B[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001B[39;00m\n\u001B[32m    768\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m769\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m exception\n",
      "\u001B[31mRuntimeError\u001B[39m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/daniilogorodnikov/PycharmProjects/RuStore-type/venv/lib/python3.13/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/Users/daniilogorodnikov/PycharmProjects/RuStore-type/venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/Users/daniilogorodnikov/PycharmProjects/RuStore-type/data/dataset.py\", line 24, in __getitem__\n    app_name_emb = embedded([X['app_name']])\n  File \"/Users/daniilogorodnikov/PycharmProjects/RuStore-type/model/embedding.py\", line 18, in embedded\n    model_output = model(**encoded_input)\n  File \"/Users/daniilogorodnikov/PycharmProjects/RuStore-type/venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/Users/daniilogorodnikov/PycharmProjects/RuStore-type/venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/Users/daniilogorodnikov/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/7710840340a098cfb869c4f65e87cf2b1b70caca/modeling_hf_nomic_bert.py\", line 1905, in forward\n    hidden_states = self.embeddings(\n        input_ids=input_ids,\n    ...<2 lines>...\n        inputs_embeds=inputs_embeds,\n    )\n  File \"/Users/daniilogorodnikov/PycharmProjects/RuStore-type/venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/Users/daniilogorodnikov/PycharmProjects/RuStore-type/venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/Users/daniilogorodnikov/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/7710840340a098cfb869c4f65e87cf2b1b70caca/modeling_hf_nomic_bert.py\", line 1010, in forward\n    embeddings = self.word_embeddings(input_ids)\n  File \"/Users/daniilogorodnikov/PycharmProjects/RuStore-type/venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/Users/daniilogorodnikov/PycharmProjects/RuStore-type/venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/Users/daniilogorodnikov/PycharmProjects/RuStore-type/venv/lib/python3.13/site-packages/torch/nn/modules/sparse.py\", line 192, in forward\n    return F.embedding(\n           ~~~~~~~~~~~^\n        input,\n        ^^^^^^\n    ...<5 lines>...\n        self.sparse,\n        ^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/daniilogorodnikov/PycharmProjects/RuStore-type/venv/lib/python3.13/site-packages/torch/nn/functional.py\", line 2546, in embedding\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Placeholder storage has not been allocated on MPS device!\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9f51e86ddeef3a6d"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
